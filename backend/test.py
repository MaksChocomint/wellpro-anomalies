# test.py
import asyncio
import pandas as pd
import numpy as np
from io import StringIO
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
import sys
import os

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –º–µ—Ç–æ–¥—ã
from methods import METHODS, z_score, lof, fft, ammad

# 12 Key drilling parameters for WellPro
REQUIRED_PARAMETERS = {
    "–≥–ª—É–±–∏–Ω–∞",                    # Depth
    "—Å–∫–æ—Ä–æ—Å—Ç—å_–±—É—Ä–µ–Ω–∏—è",           # Drilling Rate
    "–≤–µ—Å_–Ω–∞_–∫—Ä—é–∫–µ",               # Hook Load
    "–º–æ–º–µ–Ω—Ç_—Ä–æ—Ç–æ—Ä–∞",              # Torque
    "–æ–±–æ—Ä–æ—Ç—ã_—Ä–æ—Ç–æ—Ä–∞",             # RPM
    "–¥–∞–≤–ª–µ–Ω–∏–µ_–Ω–∞_–≤—Ö–æ–¥–µ",          # Inlet Pressure
    "—Ä–∞—Å—Ö–æ–¥_–Ω–∞_–≤—Ö–æ–¥–µ",            # Flow In
    "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞_–Ω–∞_–≤—ã—Ö–æ–¥–µ",      # Outlet Temperature
    "—É—Ä–æ–≤–µ–Ω—å_–≤_–µ–º–∫–æ—Å—Ç–∏",          # Tank Level
    "—Å–∫–æ—Ä–æ—Å—Ç—å_—Å–ø–æ",               # ROP SPO
    "–Ω–∞–≥—Ä—É–∑–∫–∞",                   # Weight on Bit
    "–¥–º–∫",                        # DMK
}

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
Z_SCORE_WINDOW_SIZE = 30
LOF_WINDOW_SIZE = 60    # –£–º–µ–Ω—å—à–µ–Ω –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è
FFT_WINDOW_SIZE = 64
AMMAD_WINDOW_SIZE = 40  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

Z_SCORE_THRESHOLD = 3.0
LOF_SCORE_THRESHOLD = 18.0    # –£–º–µ–Ω—å—à–µ–Ω –∏–∑-–∑–∞ –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
FFT_SCORE_THRESHOLD = 0.18    # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —à—É–º–∞
AMMAD_SCORE_THRESHOLD = 0.85

async def parse_test_data(filename: str = "default.TXT") -> Tuple[List[Dict], pd.DataFrame]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ 12 –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.
    """
    try:
        print(f"[Parser] –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {filename}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(filename):
            print(f"[Parser] –û–®–ò–ë–ö–ê: –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            current_dir = os.listdir('.')
            txt_files = [f for f in current_dir if f.endswith('.txt') or f.endswith('.TXT')]
            print(f"[Parser] –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {txt_files}")
            if txt_files:
                filename = txt_files[0]
                print(f"[Parser] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —Ñ–∞–π–ª–∞: {filename}")
        
        with open(filename, 'r', encoding='utf-8') as file:
            lines = file.readlines()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 —Å—Ç—Ä–æ–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        # –°—Ç—Ä–æ–∫–∞ 2 (–∏–Ω–¥–µ–∫—Å 2) —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏
        header_line = lines[2].strip()
        data_lines = lines[3:]  # –î–∞–Ω–Ω—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 4 —Å—Ç—Ä–æ–∫–∏
        
        print(f"[Parser] –ó–∞–≥–æ–ª–æ–≤–æ—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞: {header_line[:100]}...")
        print(f"[Parser] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö: {len(data_lines)}")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.read_csv(
            StringIO('\n'.join([header_line] + data_lines)),
            sep='\t',
            header=0,
            decimal=',',
            dtype=float,
            on_bad_lines='skip'  # –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        df.columns = df.columns.str.lower().str.strip()
        
        print(f"[Parser] –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df.columns)}")
        print(f"[Parser] –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã + –≤—Ä–µ–º—è
        valid_columns = ['–≤—Ä–µ–º—è'] + [col for col in REQUIRED_PARAMETERS if col in df.columns]
        missing_params = REQUIRED_PARAMETERS - set(df.columns)
        
        if missing_params:
            print(f"[Parser] –í–ù–ò–ú–ê–ù–ò–ï! –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {missing_params}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—å –∫–∞–∫–∏–µ-—Ç–æ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if len(valid_columns) <= 1:  # —Ç–æ–ª—å–∫–æ '–≤—Ä–µ–º—è'
            print(f"[Parser] –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤ –¥–∞–Ω–Ω—ã—Ö!")
            print(f"[Parser] –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏...")
            valid_columns = ['–≤—Ä–µ–º—è'] + [col for col in df.columns if col != '–≤—Ä–µ–º—è']
        
        df = df[valid_columns]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
        if '–≤—Ä–µ–º—è' not in df.columns:
            print(f"[Parser] –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–æ–Ω–∫–∞ '–≤—Ä–µ–º—è' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥")
            df['–≤—Ä–µ–º—è'] = np.arange(len(df))
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
        data_records = df.to_dict(orient='records')
        
        print(f"[Parser] –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(data_records)} –∑–∞–ø–∏—Å–µ–π")
        print(f"[Parser] –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {[col for col in df.columns if col != '–≤—Ä–µ–º—è']}")
        print(f"[Parser] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(df.columns) - 1}")
        
        # –í—ã–≤–æ–¥–∏–º –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö
        print("\n[Parser] –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö:")
        print(df.head())
        print(f"\n[Parser] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
        print(df.describe())
        
        return data_records, df
        
    except Exception as e:
        print(f"[Parser] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        traceback.print_exc()
        raise

async def test_z_score_method(data: pd.DataFrame, window_size: int = Z_SCORE_WINDOW_SIZE, threshold: float = Z_SCORE_THRESHOLD) -> Tuple[int, List[int], Dict]:
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ Z-score –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–æ–º–∞–ª–∏–π, –∏–Ω–¥–µ–∫—Å—ã –∞–Ω–æ–º–∞–ª–∏–π, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º)
    """
    print(f"\n[Test] –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ Z-score (–æ–∫–Ω–æ={window_size}, –ø–æ—Ä–æ–≥={threshold})...")
    
    anomalies_count = 0
    anomaly_indices = []
    total_processed = 0
    results_by_param = {}
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    for column in data.columns:
        if column == '–≤—Ä–µ–º—è' or column not in REQUIRED_PARAMETERS:
            continue
            
        print(f"  –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {column}")
        column_data = data[column].dropna().tolist()
        
        if len(column_data) <= window_size:
            print(f"    –ü—Ä–æ–ø—É—Å–∫: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(column_data)} –∑–∞–ø–∏—Å–µ–π, —Ç—Ä–µ–±—É–µ—Ç—Å—è > {window_size})")
            continue
        
        param_anomalies = 0
        param_total = 0
        
        for i in range(window_size, len(column_data)):
            window_data = column_data[i-window_size:i]
            current_value = column_data[i]
            
            is_anomaly = await z_score(
                data=window_data + [current_value],
                window_size=window_size,
                score_threshold=threshold
            )
            
            total_processed += 1
            param_total += 1
            
            if is_anomaly:
                anomalies_count += 1
                param_anomalies += 1
                anomaly_indices.append((column, i))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É
        if param_total > 0:
            results_by_param[column] = {
                'anomalies': param_anomalies,
                'total': param_total,
                'percentage': (param_anomalies / param_total) * 100
            }
            print(f"    –ê–Ω–æ–º–∞–ª–∏–π: {param_anomalies} ({results_by_param[column]['percentage']:.2f}%)")
    
    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_processed}")
    print(f"  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomalies_count}")
    print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π: {(anomalies_count/max(total_processed, 1))*100:.2f}%")
    
    return anomalies_count, anomaly_indices, results_by_param

async def test_lof_method(data: pd.DataFrame, window_size: int = LOF_WINDOW_SIZE, threshold: float = LOF_SCORE_THRESHOLD) -> Tuple[int, List[int], Dict]:
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ LOF –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    print(f"\n[Test] –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ LOF (–æ–∫–Ω–æ={window_size}, –ø–æ—Ä–æ–≥={threshold})...")
    
    anomalies_count = 0
    anomaly_indices = []
    total_processed = 0
    results_by_param = {}
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    for column in data.columns:
        if column == '–≤—Ä–µ–º—è' or column not in REQUIRED_PARAMETERS:
            continue
            
        print(f"  –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {column}")
        column_data = data[column].dropna().tolist()
        
        if len(column_data) <= window_size:
            print(f"    –ü—Ä–æ–ø—É—Å–∫: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(column_data)} –∑–∞–ø–∏—Å–µ–π, —Ç—Ä–µ–±—É–µ—Ç—Å—è > {window_size})")
            continue
        
        param_anomalies = 0
        param_total = 0
        
        # –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é N-—é —Ç–æ—á–∫—É
        step = max(1, len(column_data) // 500)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä–æ–∫
        
        for i in range(window_size, len(column_data), step):
            window_data = column_data[i-window_size:i]
            current_value = column_data[i]
            
            is_anomaly = await lof(
                data=window_data + [current_value],
                window_size=window_size,
                score_threshold=threshold
            )
            
            total_processed += 1
            param_total += 1
            
            if is_anomaly:
                anomalies_count += 1
                param_anomalies += 1
                anomaly_indices.append((column, i))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É
        if param_total > 0:
            results_by_param[column] = {
                'anomalies': param_anomalies,
                'total': param_total,
                'percentage': (param_anomalies / param_total) * 100
            }
            print(f"    –ê–Ω–æ–º–∞–ª–∏–π: {param_anomalies} ({results_by_param[column]['percentage']:.2f}%)")
    
    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_processed}")
    print(f"  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomalies_count}")
    print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π: {(anomalies_count/max(total_processed, 1))*100:.2f}%")
    
    return anomalies_count, anomaly_indices, results_by_param

async def test_fft_method(data: pd.DataFrame, window_size: int = FFT_WINDOW_SIZE, threshold: float = FFT_SCORE_THRESHOLD) -> Tuple[int, List[int], Dict]:
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ FFT –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    print(f"\n[Test] –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ FFT (–æ–∫–Ω–æ={window_size}, –ø–æ—Ä–æ–≥={threshold})...")
    
    anomalies_count = 0
    anomaly_indices = []
    total_processed = 0
    results_by_param = {}
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    for column in data.columns:
        if column == '–≤—Ä–µ–º—è' or column not in REQUIRED_PARAMETERS:
            continue
            
        print(f"  –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {column}")
        column_data = data[column].dropna().tolist()
        
        if len(column_data) < window_size:
            print(f"    –ü—Ä–æ–ø—É—Å–∫: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(column_data)} –∑–∞–ø–∏—Å–µ–π, —Ç—Ä–µ–±—É–µ—Ç—Å—è >= {window_size})")
            continue
        
        param_anomalies = 0
        param_total = 0
        
        # –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å —à–∞–≥–æ–º
        step = max(1, len(column_data) // 200)
        
        for i in range(window_size, len(column_data), step):
            window_data = column_data[i-window_size:i]
            current_value = column_data[i]
            
            is_anomaly = await fft(
                data=window_data + [current_value],
                window_size=window_size,
                score_threshold=threshold
            )
            
            total_processed += 1
            param_total += 1
            
            if is_anomaly:
                anomalies_count += 1
                param_anomalies += 1
                anomaly_indices.append((column, i))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É
        if param_total > 0:
            results_by_param[column] = {
                'anomalies': param_anomalies,
                'total': param_total,
                'percentage': (param_anomalies / param_total) * 100
            }
            print(f"    –ê–Ω–æ–º–∞–ª–∏–π: {param_anomalies} ({results_by_param[column]['percentage']:.2f}%)")
    
    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_processed}")
    print(f"  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomalies_count}")
    print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π: {(anomalies_count/max(total_processed, 1))*100:.2f}%")
    
    return anomalies_count, anomaly_indices, results_by_param

async def test_ammad_method(data: pd.DataFrame, window_size: int = AMMAD_WINDOW_SIZE, threshold: float = AMMAD_SCORE_THRESHOLD) -> Tuple[int, List[int], Dict]:
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ AMMAD –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    print(f"\n[Test] –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ AMMAD (–æ–∫–Ω–æ={window_size}, –ø–æ—Ä–æ–≥={threshold})...")
    
    anomalies_count = 0
    anomaly_indices = []
    total_processed = 0
    results_by_param = {}
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    for column in data.columns:
        if column == '–≤—Ä–µ–º—è' or column not in REQUIRED_PARAMETERS:
            continue
            
        print(f"  –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {column}")
        column_data = data[column].dropna().tolist()
        
        if len(column_data) < 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–ª—è AMMAD
            print(f"    –ü—Ä–æ–ø—É—Å–∫: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(column_data)} –∑–∞–ø–∏—Å–µ–π, —Ç—Ä–µ–±—É–µ—Ç—Å—è >= 20)")
            continue
        
        param_anomalies = 0
        param_total = 0
        
        # –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å —à–∞–≥–æ–º
        step = max(1, len(column_data) // 200)
        
        for i in range(window_size, len(column_data), step):
            window_data = column_data[i-window_size:i]
            current_value = column_data[i]
            
            is_anomaly = await ammad(
                data=window_data + [current_value],
                window_size=window_size,
                score_threshold=threshold,
                param_name=column
            )
            
            total_processed += 1
            param_total += 1
            
            if is_anomaly:
                anomalies_count += 1
                param_anomalies += 1
                anomaly_indices.append((column, i))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É
        if param_total > 0:
            results_by_param[column] = {
                'anomalies': param_anomalies,
                'total': param_total,
                'percentage': (param_anomalies / param_total) * 100
            }
            print(f"    –ê–Ω–æ–º–∞–ª–∏–π: {param_anomalies} ({results_by_param[column]['percentage']:.2f}%)")
    
    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_processed}")
    print(f"  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomalies_count}")
    print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π: {(anomalies_count/max(total_processed, 1))*100:.2f}%")
    
    return anomalies_count, anomaly_indices, results_by_param

async def analyze_parameter_statistics(data: pd.DataFrame):
    """
    –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    print("\n" + "="*60)
    print("–°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –ö–õ–Æ–ß–ï–í–´–• –ü–ê–†–ê–ú–ï–¢–†–û–í")
    print("="*60)
    
    stats = []
    for column in data.columns:
        if column == '–≤—Ä–µ–º—è' or column not in REQUIRED_PARAMETERS:
            continue
            
        values = data[column].dropna()
        if len(values) == 0:
            continue
        
        mean_val = values.mean()
        std_val = values.std()
        cv = std_val / (abs(mean_val) + 1e-10)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ CV
        if cv < 0.1:
            stability = "–æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
        elif cv < 0.5:
            stability = "–≤—ã—Å–æ–∫–∞—è"
        elif cv < 1.0:
            stability = "—Å—Ä–µ–¥–Ω—è—è"
        elif cv < 2.0:
            stability = "–Ω–∏–∑–∫–∞—è"
        else:
            stability = "–æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è"
        
        stats.append({
            '–ü–∞—Ä–∞–º–µ—Ç—Ä': column,
            '–ö–æ–ª-–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π': len(values),
            '–°—Ä–µ–¥–Ω–µ–µ': mean_val,
            '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ': std_val,
            '–ú–∏–Ω–∏–º—É–º': values.min(),
            '–ú–∞–∫—Å–∏–º—É–º': values.max(),
            '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏': cv,
            '–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å': stability,
            '–ü—Ä–æ–ø—É—Å–∫–∏ (%)': (data[column].isna().sum() / len(data)) * 100
        })
    
    if not stats:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º!")
        return None
    
    stats_df = pd.DataFrame(stats)
    print(stats_df.to_string())
    
    return stats_df

def visualize_anomaly_distribution(results: Dict, filename: str = "anomaly_distribution.png"):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –º–µ—Ç–æ–¥–∞–º.
    """
    methods = list(results.keys())
    anomaly_counts = [results[method]['anomalies_count'] for method in methods]
    total_processed = [results[method]['total_processed'] for method in methods]
    percentages = [(results[method]['anomalies_count'] / max(results[method]['total_processed'], 1)) * 100 
                   for method in methods]
    
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 15))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–æ–º–∞–ª–∏–π
    bars1 = ax1.bar(methods, anomaly_counts, color=['blue', 'green', 'orange', 'red'])
    ax1.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –º–µ—Ç–æ–¥–∞–º (12 –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)', fontsize=14, fontweight='bold')
    ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–æ–º–∞–ª–∏–π', fontsize=12)
    ax1.grid(axis='y', alpha=0.3)
    
    for bar, count in zip(bars1, anomaly_counts):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                str(count), ha='center', va='bottom', fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π
    bars2 = ax2.bar(methods, percentages, color=['lightblue', 'lightgreen', 'wheat', 'lightcoral'])
    ax2.set_title('–ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –∑–∞–ø–∏—Å–µ–π', fontsize=14, fontweight='bold')
    ax2.set_ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π (%)', fontsize=12)
    ax2.set_ylim(0, max(percentages) * 1.2)
    ax2.grid(axis='y', alpha=0.3)
    
    for bar, perc in zip(bars2, percentages):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{perc:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º (–¥–ª—è AMMAD)
    if 'AMMAD' in results and 'results_by_param' in results['AMMAD']:
        param_results = results['AMMAD']['results_by_param']
        if param_results:
            params = list(param_results.keys())
            param_percentages = [param_results[p]['percentage'] for p in params]
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –∞–Ω–æ–º–∞–ª–∏–π
            sorted_indices = np.argsort(param_percentages)[::-1]
            params = [params[i] for i in sorted_indices]
            param_percentages = [param_percentages[i] for i in sorted_indices]
            
            # –ë–µ—Ä–µ–º —Ç–æ–ø-10 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            top_n = min(10, len(params))
            ax3.barh(params[:top_n], param_percentages[:top_n], color='steelblue')
            ax3.set_title(f'–¢–æ–ø-{top_n} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É –∞–Ω–æ–º–∞–ª–∏–π (AMMAD)', fontsize=14, fontweight='bold')
            ax3.set_xlabel('–ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π (%)')
            ax3.grid(axis='x', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(filename, dpi=150, bbox_inches='tight')
    print(f"\n[Visualization] –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {filename}")

def save_test_results(results: Dict, stats_df: pd.DataFrame, filename: str = "test_results.json"):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ JSON —Ñ–∞–π–ª.
    """
    result_data = {
        'timestamp': datetime.now().isoformat(),
        'tested_parameters': list(REQUIRED_PARAMETERS),
        'test_configuration': {
            'z_score': {'window_size': Z_SCORE_WINDOW_SIZE, 'threshold': Z_SCORE_THRESHOLD},
            'lof': {'window_size': LOF_WINDOW_SIZE, 'threshold': LOF_SCORE_THRESHOLD},
            'fft': {'window_size': FFT_WINDOW_SIZE, 'threshold': FFT_SCORE_THRESHOLD},
            'ammad': {'window_size': AMMAD_WINDOW_SIZE, 'threshold': AMMAD_SCORE_THRESHOLD},
        },
        'methods': {},
        'statistics_summary': {
            'total_parameters_tested': len(stats_df) if stats_df is not None else 0,
            'average_records_per_param': stats_df['–ö–æ–ª-–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π'].mean() if stats_df is not None else 0,
        }
    }
    
    for method, result in results.items():
        method_data = {
            'anomalies_count': result['anomalies_count'],
            'total_processed': result['total_processed'],
            'anomaly_percentage': result['anomaly_percentage'],
            'window_size': result['window_size'],
            'threshold': result['threshold']
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'results_by_param' in result:
            method_data['results_by_param'] = result['results_by_param']
        
        result_data['methods'][method] = method_data
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\n[Results] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filename}")
    return result_data

def calculate_total_processed(data: pd.DataFrame, window_size: int) -> int:
    """
    –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    total = 0
    for column in data.columns:
        if column == '–≤—Ä–µ–º—è' or column not in REQUIRED_PARAMETERS:
            continue
            
        column_data = data[column].dropna()
        if len(column_data) > window_size:
            total += len(column_data) - window_size
    
    return total

async def run_comprehensive_test(data_file: str = "default.TXT"):
    """
    –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    print("="*80)
    print("–ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–ï–¢–û–î–û–í –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø –ê–ù–û–ú–ê–õ–ò–ô")
    print("–¢–æ–ª—å–∫–æ 12 –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±—É—Ä–µ–Ω–∏—è")
    print("="*80)
    
    print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {REQUIRED_PARAMETERS}")
    print(f"\n–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"  Z-score: –æ–∫–Ω–æ={Z_SCORE_WINDOW_SIZE}, –ø–æ—Ä–æ–≥={Z_SCORE_THRESHOLD}")
    print(f"  LOF: –æ–∫–Ω–æ={LOF_WINDOW_SIZE}, –ø–æ—Ä–æ–≥={LOF_SCORE_THRESHOLD}")
    print(f"  FFT: –æ–∫–Ω–æ={FFT_WINDOW_SIZE}, –ø–æ—Ä–æ–≥={FFT_SCORE_THRESHOLD}")
    print(f"  AMMAD: –æ–∫–Ω–æ={AMMAD_WINDOW_SIZE}, –ø–æ—Ä–æ–≥={AMMAD_SCORE_THRESHOLD}")
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\n" + "="*80)
        print("[Step 1] –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        data_records, df = await parse_test_data(data_file)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
        available_params = [col for col in REQUIRED_PARAMETERS if col in df.columns]
        print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –¥–∞–Ω–Ω—ã—Ö: {available_params}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(available_params)}")
        
        if len(available_params) == 0:
            print("–û–®–ò–ë–ö–ê: –í –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞!")
            return None, None
        
        # 2. –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
        print("\n" + "="*80)
        print("[Step 2] –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
        stats_df = await analyze_parameter_statistics(df)
        
        if stats_df is None:
            print("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫!")
            return None, None
        
        # 3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
        print("\n" + "="*80)
        print("[Step 3] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π...")
        
        test_results = {}
        
        # Z-score —Ç–µ—Å—Ç
        print("\n" + "-"*40)
        z_anomalies, z_indices, z_results_by_param = await test_z_score_method(
            df, 
            window_size=Z_SCORE_WINDOW_SIZE, 
            threshold=Z_SCORE_THRESHOLD
        )
        test_results['Z-score'] = {
            'anomalies_count': z_anomalies,
            'anomaly_indices': z_indices,
            'total_processed': calculate_total_processed(df, Z_SCORE_WINDOW_SIZE),
            'anomaly_percentage': (z_anomalies / max(calculate_total_processed(df, Z_SCORE_WINDOW_SIZE), 1)) * 100,
            'window_size': Z_SCORE_WINDOW_SIZE,
            'threshold': Z_SCORE_THRESHOLD,
            'results_by_param': z_results_by_param
        }
        
        # LOF —Ç–µ—Å—Ç
        print("\n" + "-"*40)
        lof_anomalies, lof_indices, lof_results_by_param = await test_lof_method(
            df,
            window_size=LOF_WINDOW_SIZE,
            threshold=LOF_SCORE_THRESHOLD
        )
        test_results['LOF'] = {
            'anomalies_count': lof_anomalies,
            'anomaly_indices': lof_indices,
            'total_processed': calculate_total_processed(df, LOF_WINDOW_SIZE),
            'anomaly_percentage': (lof_anomalies / max(calculate_total_processed(df, LOF_WINDOW_SIZE), 1)) * 100,
            'window_size': LOF_WINDOW_SIZE,
            'threshold': LOF_SCORE_THRESHOLD,
            'results_by_param': lof_results_by_param
        }
        
        # FFT —Ç–µ—Å—Ç
        print("\n" + "-"*40)
        fft_anomalies, fft_indices, fft_results_by_param = await test_fft_method(
            df,
            window_size=FFT_WINDOW_SIZE,
            threshold=FFT_SCORE_THRESHOLD
        )
        test_results['FFT'] = {
            'anomalies_count': fft_anomalies,
            'anomaly_indices': fft_indices,
            'total_processed': calculate_total_processed(df, FFT_WINDOW_SIZE),
            'anomaly_percentage': (fft_anomalies / max(calculate_total_processed(df, FFT_WINDOW_SIZE), 1)) * 100,
            'window_size': FFT_WINDOW_SIZE,
            'threshold': FFT_SCORE_THRESHOLD,
            'results_by_param': fft_results_by_param
        }
        
        # AMMAD —Ç–µ—Å—Ç
        print("\n" + "-"*40)
        ammad_anomalies, ammad_indices, ammad_results_by_param = await test_ammad_method(
            df,
            window_size=AMMAD_WINDOW_SIZE,
            threshold=AMMAD_SCORE_THRESHOLD
        )
        test_results['AMMAD'] = {
            'anomalies_count': ammad_anomalies,
            'anomaly_indices': ammad_indices,
            'total_processed': calculate_total_processed(df, AMMAD_WINDOW_SIZE),
            'anomaly_percentage': (ammad_anomalies / max(calculate_total_processed(df, AMMAD_WINDOW_SIZE), 1)) * 100,
            'window_size': AMMAD_WINDOW_SIZE,
            'threshold': AMMAD_SCORE_THRESHOLD,
            'results_by_param': ammad_results_by_param
        }
        
        # 4. –í—ã–≤–æ–¥ —Å–≤–æ–¥–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "="*80)
        print("–°–í–û–î–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
        print("12 –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±—É—Ä–µ–Ω–∏—è")
        print("="*80)
        
        summary_df = pd.DataFrame([
            {
                '–ú–µ—Ç–æ–¥': method,
                '–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π': results['total_processed'],
                '–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π': results['anomalies_count'],
                '–ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π (%)': f"{results['anomaly_percentage']:.2f}%",
                '–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞': results['window_size'],
                '–ü–æ—Ä–æ–≥': results['threshold']
            }
            for method, results in test_results.items()
        ])
        
        print(summary_df.to_string(index=False))
        
        # 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        print("\n" + "="*80)
        print("[Step 4] –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        visualize_anomaly_distribution(test_results)
        
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "="*80)
        print("[Step 5] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        save_test_results(test_results, stats_df)
        
        # 7. –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        print("\n" + "="*80)
        print("[Step 6] –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º...")
        await detailed_parameter_analysis(df, test_results)
        
        print("\n" + "="*80)
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("="*80)
        
        return test_results, stats_df
        
    except Exception as e:
        print(f"\n[Error] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        return None, None

async def detailed_parameter_analysis(df: pd.DataFrame, test_results: Dict):
    """
    –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –∫–∞–∂–¥–æ–º—É –ø–∞—Ä–∞–º–µ—Ç—Ä—É.
    """
    print("\n" + "-"*60)
    print("–î–ï–¢–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û –ü–ê–†–ê–ú–ï–¢–†–ê–ú")
    print("-"*60)
    
    results_by_param = {}
    
    for param in REQUIRED_PARAMETERS:
        if param not in df.columns:
            continue
            
        print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä: {param}")
        param_data = df[param].dropna().tolist()
        
        if len(param_data) < 100:
            print(f"  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(param_data)} –∑–∞–ø–∏—Å–µ–π")
            continue
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
        method_results = {}
        for method_name in ['Z-score', 'LOF', 'FFT', 'AMMAD']:
            if method_name in test_results and 'results_by_param' in test_results[method_name]:
                if param in test_results[method_name]['results_by_param']:
                    method_results[method_name] = test_results[method_name]['results_by_param'][param]
        
        if method_results:
            print(f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(param_data)}")
            for method_name, results in method_results.items():
                print(f"  {method_name:10}: {results['anomalies']:4} ({results['percentage']:.1f}%)")
        else:
            print(f"  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        results_by_param[param] = method_results
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if results_by_param:
        with open("detailed_results.json", "w", encoding="utf-8") as f:
            json.dump(results_by_param, f, ensure_ascii=False, indent=2, default=str)
        print(f"\n[Results] –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: detailed_results.json")

async def test_specific_parameters(data_file: str = "default.TXT", params: List[str] = None):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    print("\n" + "="*60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–û–ù–ö–†–ï–¢–ù–´–• –ö–õ–Æ–ß–ï–í–´–• –ü–ê–†–ê–ú–ï–¢–†–û–í")
    print("="*60)
    
    try:
        _, df = await parse_test_data(data_file)
        
        if params is None:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            params = [p for p in REQUIRED_PARAMETERS if p in df.columns]
        
        if not params:
            print("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è!")
            return
        
        print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}")
        
        for param in params:
            if param not in df.columns:
                print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä '{param}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö")
                continue
                
            print(f"\n" + "="*40)
            print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä: {param}")
            print(f"  –í—Å–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–π: {len(df[param].dropna())}")
            print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: {df[param].min():.2f} - {df[param].max():.2f}")
            print(f"  –°—Ä–µ–¥–Ω–µ–µ: {df[param].mean():.2f}, –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {df[param].std():.2f}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã –Ω–∞ —ç—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–µ
            param_data = df[[param]].dropna()
            
            if len(param_data) < 100:
                print(f"  –ü—Ä–æ–ø—É—Å–∫: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                continue
            
            # Z-score
            z_anomalies = 0
            z_total = max(len(param_data) - Z_SCORE_WINDOW_SIZE, 1)
            for i in range(Z_SCORE_WINDOW_SIZE, len(param_data), 10):  # –®–∞–≥ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                window = param_data.iloc[i-Z_SCORE_WINDOW_SIZE:i][param].tolist()
                current = param_data.iloc[i][param]
                
                is_anomaly = await z_score(window + [current], 
                                         window_size=Z_SCORE_WINDOW_SIZE, 
                                         score_threshold=Z_SCORE_THRESHOLD)
                if is_anomaly:
                    z_anomalies += 1
            
            # LOF
            lof_anomalies = 0
            lof_total = max(len(param_data) - LOF_WINDOW_SIZE, 1)
            for i in range(LOF_WINDOW_SIZE, len(param_data), 20):  # –ë–æ–ª—å—à–∏–π —à–∞–≥, —Ç.–∫. LOF –º–µ–¥–ª–µ–Ω–Ω–µ–µ
                window = param_data.iloc[i-LOF_WINDOW_SIZE:i][param].tolist()
                current = param_data.iloc[i][param]
                
                is_anomaly = await lof(window + [current], 
                                     window_size=LOF_WINDOW_SIZE, 
                                     score_threshold=LOF_SCORE_THRESHOLD)
                if is_anomaly:
                    lof_anomalies += 1
            
            # FFT
            fft_anomalies = 0
            fft_total = max(len(param_data) - FFT_WINDOW_SIZE, 1)
            for i in range(FFT_WINDOW_SIZE, len(param_data), 10):
                window = param_data.iloc[i-FFT_WINDOW_SIZE:i][param].tolist()
                current = param_data.iloc[i][param]
                
                is_anomaly = await fft(window + [current], 
                                     window_size=FFT_WINDOW_SIZE, 
                                     score_threshold=FFT_SCORE_THRESHOLD)
                if is_anomaly:
                    fft_anomalies += 1
            
            # AMMAD
            ammad_anomalies = 0
            ammad_total = max(len(param_data) - AMMAD_WINDOW_SIZE, 1)
            for i in range(AMMAD_WINDOW_SIZE, len(param_data), 10):
                window = param_data.iloc[i-AMMAD_WINDOW_SIZE:i][param].tolist()
                current = param_data.iloc[i][param]
                
                is_anomaly = await ammad(window + [current], 
                                       window_size=AMMAD_WINDOW_SIZE, 
                                       score_threshold=AMMAD_SCORE_THRESHOLD,
                                       param_name=param)
                if is_anomaly:
                    ammad_anomalies += 1
            
            print(f"  Z-score:  {z_anomalies:4} ({z_anomalies/z_total*100:.1f}%)")
            print(f"  LOF:      {lof_anomalies:4} ({lof_anomalies/lof_total*100:.1f}%)")
            print(f"  FFT:      {fft_anomalies:4} ({fft_anomalies/fft_total*100:.1f}%)")
            print(f"  AMMAD:    {ammad_anomalies:4} ({ammad_anomalies/ammad_total*100:.1f}%)")
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
        import traceback
        traceback.print_exc()

def print_test_summary():
    """
    –ü–µ—á–∞—Ç—å —Å–≤–æ–¥–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    print("="*80)
    print("–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ê–ù–û–ú–ê–õ–ò–ô")
    print("="*80)
    print(f"\nüìä –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–µ—Ç–æ–¥–æ–≤:")
    print(f"  Z-score:    –æ–∫–Ω–æ={Z_SCORE_WINDOW_SIZE}, –ø–æ—Ä–æ–≥={Z_SCORE_THRESHOLD}")
    print(f"  LOF:        –æ–∫–Ω–æ={LOF_WINDOW_SIZE}, –ø–æ—Ä–æ–≥={LOF_SCORE_THRESHOLD}")
    print(f"  FFT:        –æ–∫–Ω–æ={FFT_WINDOW_SIZE}, –ø–æ—Ä–æ–≥={FFT_SCORE_THRESHOLD}")
    print(f"  AMMAD:      –æ–∫–Ω–æ={AMMAD_WINDOW_SIZE}, –ø–æ—Ä–æ–≥={AMMAD_SCORE_THRESHOLD}")
    
    print(f"\nüéØ 12 –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±—É—Ä–µ–Ω–∏—è:")
    for i, param in enumerate(sorted(REQUIRED_PARAMETERS), 1):
        print(f"  {i:2}. {param}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±—É—Ä–µ–Ω–∏—è')
    parser.add_argument('--file', type=str, default='default.TXT', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('--specific', action='store_true', help='–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã')
    parser.add_argument('--params', type=str, nargs='+', help='–°–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--summary', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å–≤–æ–¥–∫—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    if args.summary:
        print_test_summary()
    elif args.specific:
        asyncio.run(test_specific_parameters(args.file, args.params))
    else:
        asyncio.run(run_comprehensive_test(args.file))